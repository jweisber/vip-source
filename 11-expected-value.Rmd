# (PART\*) Part II {-}

# Expected Value

We've analyzed reasoning about what's true, or at least probable. But a lot of reasoning is *practical*: it's about what the best course of action is. Once you know how likely the various possible outcomes of a decision are, how do you make your choice?

```{block, type='problem'}
You have a free ticket to play one game at the local casino. You can choose either The Coin Game or The Die Game.

- The Coin Game: a fair coin is flipped. You win $\$1$ if it lands heads, nothing if it lands tails.

- The Die Game: a fair die is rolled. You win $\$3$ if it lands five or six, nothing otherwise.

Which game should you play?
```

The coin game is guaranteed to win you money, while the die game only has a $1/3$ chance of paying off. On the other hand, the die game promises a bigger payoff, $\$4$ rather than just $\$1$.

So there are conflicting arguments here. The odds favour the first choice, while the potential payoffs favour the second. How do you reconcile these two arguments and come to a decision?

Well, let's think about what would happen in the long run if you played each game over and over.

Suppose you played the coin game over and over, a hundred times. Half the time you'd win $\$1$, and half the time you'd win nothing. So after a hundred plays, you'd probably win about $\$50$, which is an average of $\$0.50$ per play.

Now suppose you play the die game a hundred times instead. A third of the time you'd win $\$3$, and the other two thirds of the time you'd win nothing. So after a hundred plays you'd have won about $\$100$, an average of about $\$1$ per play.

In technical terms, this means The Die Game has a higher "expected value". In the long run, you'd expect to win about $\$1$ per play in The Die Game, as opposed to $\$0.50$ for The Coin Game.

So The Die Game is the more advantageous one, on this analysis. On average, people tend to win more playing that game.


## Expected Monetary Values

To calculate the expected value of an option, we multiply each payoff by its probability, and then sum up the results. For example, our analysis of The Coin Game can be written this way:
$$
  \begin{aligned}
    \p(\$1) \times \$1 + \p(\$0) \times \$0 &= (1/2)(\$1) + (1/2)(\$0)\\
      &= \$0.50.
  \end{aligned}
$$
And for The Die Game:
$$
  \begin{aligned}
    \p(\$3) \times \$3 + \p(\$0) \times \$0 &= (1/3)(\$3) + (2/3)(\$0)\\
      &= \$1.
  \end{aligned}
$$

Sometimes a gamble has the potential to lose money, so we use a negative number for the "payoff". For example, suppose The Die Game is modified so that you have to pay $\$3$ if the die lands $1, 2, 3$, or $4$. Then we calculate:
$$
  \begin{aligned}
    \p(\$3) \times \$3 + \p(-\$3) \times -\$1 &= (1/3)(\$3) + (2/3)(-\$3)\\
      &= -\$1.
  \end{aligned}
$$

In general, the formula for a game with possible payoffs $\$x$ and $\$y$ is:
$$ \p(\$x) \times \$x + \p(\$y) \times \$y. $$

Notice that expected values aren't necessarily what you might think from the name. Th expected value of a game isn't necessarily the amount you expect to win playing it. If you play The Die Game, you'll either win $\$3$ or nothing. But the expected value of $\$1$. So you actually know for certain ahead of time that you won't win the expected value, $\$1$.

```{block, type='warning'}
The expected value isn't the amount you expect to win from a game. It's the amount you expect to win *on average, in the long run*, if you play the game over and over.
```


## Visualizing Expectations

Long run averages aren't the only way to help make expectations intuitive. You can also visualize them as the area of a diagram like this one:

```{r echo=FALSE, fig.cap="Expected value as area"}
knitr::include_graphics("img/fig.png")
```

The $x$-axis here represents probability, the $y$-axis is money. Each possible outcome corresponds to a rectangular region in the diagram. The rectangle's width is the probability of that outcome occurring, and the height is the payoff. So the rectangle's area is a term in the sum for the expected value. The total expected value is thus the area of all the rectangles.

Notice how a rectangle's area can enlarged by increasing either its width or its height (or both). Likewise, a payoff's contribution to expected value can be increased either by increasing its amount or by increasing its probability (or both).

We started this chapter wondering how to reconcile conflicting arguments for and against making a certain choice. One choice had better odds, the other had a greater potential payoff. The expected value formula answers the question by giving equal weight to these competing reasons. Width (odds) and height (payoff) are treated the same: neither one is given greater weight. We just multiply one against the other and take the total area that results.

```{r echo=FALSE, fig.margin = TRUE, fig.cap="A gamble with one negative payoff"}
knitr::include_graphics("img/marg_fig.png")
```

Negative outcomes are represented by rectangles below the $x$-axis. We colour these red, as a reminder that they are to be subtracted from the area above the $x$-axis. So to get the expected value, we subtract the red area from the blue area.

```{r echo=FALSE, fig.margin = TRUE, fig.cap="A gamble with four possible outcomes"}
knitr::include_graphics("img/marg_fig.png")
```

Gambles with three or more outcomes can be represented similarly. We just use three or more reactangles, one for each possible outcome.


## Fair Prices

Imagine a variation of The Coin Game, where you win $\$1$ if the coin lands heads, and you pay $\$1$ if it lands tails. You might be able to guess, the expected value of this game is $\$0$:
$$
  \begin{aligned}
    \p(\$1) \times \$1 + \p(-\$1) \times -\$1 &= (1/2)(\$1) + (1/2)(-\$1)\\
      &= 0.
  \end{aligned}
$$
If you played over and over again, you'd break even in the long run. Half the time you'd win a dollar, and the other half of the time you'd pay a dollar back.

So if someone asked you to pay to play this game, that would seem unfair. Why pay to play if you don't expect to win?

But for a game like The Die Game, asking people to pay to play does seem fair. In the long run, the casino is going to lose an average of $\$1$ per play. So it's only fair that they ask people to pay $\$1$ to play that game. In the long run they'll break even. (Of course, a real casino will demand more so that they can turn a profit.)

In general, a game's fair price is thought to be its expected value. If the expected value is $\$3$, it's fair to pay $\$3$ to play. (If the expected value is $-\$3$, it's fair to "pay" $\$3$ to play: meaning *they should pay you* $\$3$ to play.)

Notice that the expected value of paying the fair price is always $\$0$. Suppose you pay $\$1$ to play The Die Game, for example. Because \$1 is automatically deducted from your winnings, the possible consequences are now $\$3 - \$1 = \$2$ and $\$0 - \$1 = -\$1$. So the expected value of paying \$1 to play is:
$$
  \begin{aligned}
    (1/3)(\$2) + (2/3)(-\$1) &= \$2/3 - \$2/3\\
      &= \$0.
  \end{aligned}
$$

That makes sense: if it's a fair price, you shouldn't have any advantage over the casino. You don't stand to gain or lose money in the long run, if you pay to play over and over.

And the casino is in the same position. For them, the losses and gains are reversed: a roll of five or six loses them \$3, and otherwise they lose nothing. So if you pay them $\$1$ to play, they either lose $\$2$ total or gain $\$1$:
$$
  \begin{aligned}
    (1/3)(-\$2) + (2/3)(\$1) &= -\$2/3 + \$2/3\\
      &= \$0.
  \end{aligned}
$$

Since nobody has an advantage, it's a fair deal.


## Three Or More Consequences

Most casino games have mor than two possible payoffs. For that matter, most decisions in life have more than two possible outcomes. If you buy insurance for your phone, it could end up saving you $\$0$, or it could save you $\$100$, or it might save you $\$150$, etc.

So the official definition of expected monetary value is:

Expected Monetary Value

:   Given an act $A$ with possible consequences $\$x_1, \$x_2, \ldots, \$x_n$, the *expected monetary value* of $A$ is:
$$
  \begin{aligned}
    \E(A) &= \p(\$x_1) \times \$x_1 + \p(\$x_2) \times \$x_2 + \ldots + \p(x_n) \times \$x_n.
  \end{aligned}
$$

For example, suppose a fair die will be rolled and you will win \$1 if it lands on a low number, \$2 if it's a middling number. If it's a high number, you lose \$3. Then:
$$
  \begin{aligned}
    \E(A) &= (1/3)(\$1) + (1/3)(\$2) + (1/3)(-\$3)\\
          &= \$1/3 + \$2/3 - \$1\\
          &= \$0.
  \end{aligned}
$$


## Other Goods

Money isn't everything, of course. People value lots of other things, like their health, their comfort, and their free time. The concept of expected value can be applied to other goods besides money, as long as they're quantifiable.

```{block, type='problem'}
Imagine you're taking a trip to Ottawa and you can either drive or go by train. If you take the train, it's pretty predictable: it's always a five or six hour ride, with equal probability each way. If you drive, it's less predictable. Nine times out of ten it's just a four hour drive. But one time in ten, there's an accident and it ends up being a ten hour drive.
```

If all you care about is spending as little time on the road as possible, should you take the train or drive?
$$
  \begin{aligned}
    \E(\mbox{Train}) &= \p(-5{\mbox{ hr}}) \times -5{\mbox{ hr}}+ \p(-6{\mbox{ hr}}) \times -6{\mbox{ hr}}\\
                     &= (1/2)(-5{\mbox{ hr}}) + (1/2)(-6{\mbox{ hr}})\\
                     &= -11/2{\mbox{ hr}}\\
                     &= -5.5{\mbox{ hr}}\\
    \E(\mbox{Drive}) &= \p(-4{\mbox{ hr}}) \times -4{\mbox{ hr}}+ \p(-10{\mbox{ hr}}) \times -10{\mbox{ hr}}\\
                     &= (9/10)(-4{\mbox{ hr}}) + (1/10)(-10{\mbox{ hr}})\\
                     &= -46/10{\mbox{ hr}}\\
                     &= -4.6{\mbox{ hr}}.
  \end{aligned}
$$
So driving has a higher expected value---measured in terms of free time, rather than money.


## The St. Petersburg Game

```{r echo=FALSE, fig.cap="Nicolaus I Bernoulli (1687â€“1759)", fig.margin=TRUE}
knitr::include_graphics("img/marg_fig.png")
```

Still, the concept of expected monetary value does have its limitations. One of these limitations was discovered by Nicolas Bernoulli in the early 1700's.

```{block, type='problem'}
I'm going to flip a fair coin until it comes up heads, at which point the game ends.

- If it comes up heads on the first flip, you get $\$2$.
- If it comes up heads on the second flip, you get $\$4$.
- If it comes up heads on the third flip, you get $\$8$.
- Etc.

What is a fair price to demand for playing this game?
```

Well, how much would you be willing to pay to play this game? You probably won't win more than a few dollars. Half the time people just win $\$2$. $3/4$ of the time they only win $\$4$. Etc. So most people are only willing to pay a few dollars to play this game.

But what is the expected monetary value?
$$
  \begin{aligned}
    \E(\mbox{St. Petersburg Game}) &= \p(\$2) \times \$2 + \p(\$4) \times \$4 + \p(\$8) \times \$8 + \ldots\\
       &= (1/2)(\$2) + (1/4)(\$4) + (1/8)(\$8) + \ldots\\
       &= 1 + 1 + 1 + \ldots\\
       &= \infty.
  \end{aligned}
$$
So the game has infinite expected value! And doesn't that mean you should be willing to pay however much money you have to play?? After all, however much you pay, it's still a bargain. The fair price is $\$\infty$!

Well, maybe. We'll dig more deeply into this famous puzzle in the next chapter.
